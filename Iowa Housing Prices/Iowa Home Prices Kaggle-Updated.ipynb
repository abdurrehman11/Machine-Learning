{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filepath to variable for easier access\n",
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "# read the data and store data in DataFrame titled lowa_data\n",
    "home_data = pd.read_csv(lowa_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000   \n",
       "\n",
       "           ...         WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count      ...        1460.000000  1460.000000    1460.000000  1460.000000   \n",
       "mean       ...          94.244521    46.660274      21.954110     3.409589   \n",
       "std        ...         125.338794    66.256028      61.119149    29.317331   \n",
       "min        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "25%        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "50%        ...           0.000000    25.000000       0.000000     0.000000   \n",
       "75%        ...         168.000000    68.000000       0.000000     0.000000   \n",
       "max        ...         857.000000   547.000000     552.000000   508.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
       "mean     15.060959     2.758904     43.489041     6.321918  2007.815753   \n",
       "std      55.757415    40.177307    496.123024     2.703626     1.328095   \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000   \n",
       "25%       0.000000     0.000000      0.000000     5.000000  2007.000000   \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000   \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000   \n",
       "max     480.000000   738.000000  15500.000000    12.000000  2010.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1460.000000  \n",
       "mean   180921.195890  \n",
       "std     79442.502883  \n",
       "min     34900.000000  \n",
       "25%    129975.000000  \n",
       "50%    163000.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a summary of the data in Lowa data\n",
    "home_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg lot size =  10517\n",
      "Newest_home_age =  8.0\n"
     ]
    }
   ],
   "source": [
    "home_statistics = home_data.describe()\n",
    "\n",
    "# What is the average lot size (rounded to nearest integer)?\n",
    "avg_lot_size = int(home_statistics['LotArea'][1]) + 1\n",
    "print(\"Avg lot size = \", avg_lot_size)\n",
    "\n",
    "# As of today, how old is the newest home (current year - the date in which it was built)\n",
    "newest_home_age = 2018 - home_statistics['YearBuilt'][7]\n",
    "print(\"Newest_home_age = \", newest_home_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ================= Think About Your Data =========================\n",
    "\n",
    "# The newest house in your data isn't that new.  A few potential explanations for this:\n",
    "# 1. They haven't built new houses where this data was collected.\n",
    "# 1. The data was collected a long time ago. Houses built after the data publication wouldn't show up.\n",
    "\n",
    "# If the reason is explanation #1 above, does that affect your trust in the model you build with this data? What about\n",
    "# if it is reason #2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1681aa1e6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "home_data['YearBuilt'].value_counts().head(30).sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1681cb057f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "home_data['GarageYrBlt'].value_counts().head(30).sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1681a9f0208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "home_data['YearRemodAdd'].value_counts().head(30).sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1681cd130b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "home_data['YrSold'].value_counts().head(30).sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== Explanation of Reasons Using Bar Graphs ======================\n",
    "\n",
    "# One way to see this is to look at histograms of the data with year values (YearBuilt, YearRemodAdd, GarageYrBlt, YrSold). \n",
    "# If the histograms are right skewed (with a right tail), then it could be possible where newer houses were being \n",
    "# built/sold/remodeled less frequently in the area. If the histograms are not right skewed (more uniform/flat or left skewed), \n",
    "# then #2 becomes more likely where the data was collected sometime near 2010 with no data on houses after that.\n",
    "\n",
    "# Looking at the histograms/distribution of the data, it looks like the year data is either close to uniform (YrSold), \n",
    "# or is mostly left skewed (YearBuilt, GarageYrBlt, YearRemodAdd). This points to the data being collected a while ago \n",
    "# since there is an 'abrupt' stoppage of data at the more recent years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
      "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
      "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
      "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
      "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
      "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
      "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
      "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
      "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
      "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
      "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
      "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
      "       'SaleCondition', 'SalePrice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# ======================  Selecting Data for Modeling  ========================\n",
    "\n",
    "# To choose variables/columns, we'll need to see a list of all columns in the dataset\n",
    "print(home_data.columns)\n",
    "\n",
    "# select the column we want to predict, which is called the prediction target. \n",
    "# By convention, the prediction target is called y\n",
    "y = home_data.SalePrice\n",
    "\n",
    "# choose features\n",
    "# Create the list of features below\n",
    "feature_names = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "\n",
    "# select data corresponding to features in feature_names\n",
    "X = home_data[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10516.828082</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1162.626712</td>\n",
       "      <td>346.992466</td>\n",
       "      <td>1.565068</td>\n",
       "      <td>2.866438</td>\n",
       "      <td>6.517808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9981.264932</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>386.587738</td>\n",
       "      <td>436.528436</td>\n",
       "      <td>0.550916</td>\n",
       "      <td>0.815778</td>\n",
       "      <td>1.625393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7553.500000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9478.500000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1087.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11601.500000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1391.250000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215245.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>4692.000000</td>\n",
       "      <td>2065.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LotArea    YearBuilt     1stFlrSF     2ndFlrSF     FullBath  \\\n",
       "count    1460.000000  1460.000000  1460.000000  1460.000000  1460.000000   \n",
       "mean    10516.828082  1971.267808  1162.626712   346.992466     1.565068   \n",
       "std      9981.264932    30.202904   386.587738   436.528436     0.550916   \n",
       "min      1300.000000  1872.000000   334.000000     0.000000     0.000000   \n",
       "25%      7553.500000  1954.000000   882.000000     0.000000     1.000000   \n",
       "50%      9478.500000  1973.000000  1087.000000     0.000000     2.000000   \n",
       "75%     11601.500000  2000.000000  1391.250000   728.000000     2.000000   \n",
       "max    215245.000000  2010.000000  4692.000000  2065.000000     3.000000   \n",
       "\n",
       "       BedroomAbvGr  TotRmsAbvGrd  \n",
       "count   1460.000000   1460.000000  \n",
       "mean       2.866438      6.517808  \n",
       "std        0.815778      1.625393  \n",
       "min        0.000000      2.000000  \n",
       "25%        2.000000      5.000000  \n",
       "50%        3.000000      6.000000  \n",
       "75%        3.000000      7.000000  \n",
       "max        8.000000     14.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================  Review Data  =========================\n",
    "\n",
    "# print description or statistics from X\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8450</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9600</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11250</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9550</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14260</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
       "0     8450       2003       856       854         2             3   \n",
       "1     9600       1976      1262         0         2             3   \n",
       "2    11250       2001       920       866         2             3   \n",
       "3     9550       1915       961       756         1             3   \n",
       "4    14260       2000      1145      1053         2             4   \n",
       "\n",
       "   TotRmsAbvGrd  \n",
       "0             8  \n",
       "1             6  \n",
       "2             6  \n",
       "3             7  \n",
       "4             9  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the top few lines\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================  Building Your Model  ========================\n",
    "\n",
    "# 1 - Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters \n",
    "#     of the model type are specified too.\n",
    "# 2 - Fit: Capture patterns from provided data. This is the heart of modeling.\n",
    "# 3 - Predict: Just what it sounds like\n",
    "# 4 - Evaluate: Determine how accurate the model's predictions are.\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# specify the model. \n",
    "# For model reproducibility, set a numeric value for random_state when specifying the model\n",
    "lowa_model = DecisionTreeRegressor(random_state = 1)\n",
    "\n",
    "# Fit the model\n",
    "lowa_model.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Training Set :  [208500. 181500. 223500. ... 266500. 142125. 147500.]\n",
      "\n",
      "Acutal Output (y) :  [208500 181500 223500 ... 266500 142125 147500]\n"
     ]
    }
   ],
   "source": [
    "predictions = lowa_model.predict(X)\n",
    "print(\"Predictions on Training Set : \", predictions)\n",
    "print(\"\\nAcutal Output (y) : \", y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================  Model Validation  ======================\n",
    "\n",
    "# huge mistake when measuring predictive accuracy on train set\n",
    "# Mean Absolute Error (also called MAE) is one of the metric for summarizing model quality\n",
    "# error = actual − predicted\n",
    "# With the MAE metric, we take the absolute value of each error. This converts each error to a positive number. \n",
    "# We then take the average of those absolute errors.\n",
    "\n",
    "# measure performance on data that wasn't used to build the model. The most straightforward way \n",
    "# to do this is to exclude some data from the model-building process, and then use those to test \n",
    "# the model's accuracy on data it hasn't seen before. This data is called validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the model\n",
    "lowa_model = DecisionTreeRegressor(random_state = 1)\n",
    "\n",
    "# Fit iowa_model with the training data.\n",
    "lowa_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with all validation observations\n",
    "val_predictions = lowa_model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186500. 184000. 130000.  92000. 164500.]\n",
      "[231500 179500 122000  84500 142000]\n"
     ]
    }
   ],
   "source": [
    "# print the top few validation predictions\n",
    "print(val_predictions[0:5])\n",
    "# print the top few actual prices from validation data\n",
    "print(val_y.head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE without any optimization:  29652.931506849316\n"
     ]
    }
   ],
   "source": [
    "# ====================  Calculate the Mean Absolute Error in Validation Data  ==================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "\n",
    "print(\"MAE without any optimization: \", val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================  Overfitting and Underfitting  ======================\n",
    "\n",
    "# The phenomenon is called overfitting, where a model matches the training data almost perfectly, \n",
    "# but does poorly in validation and other new data.\n",
    "\n",
    "# When a model fails to capture important distinctions and patterns in the data, so it performs \n",
    "# poorly even in training data, that is called underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. \n",
    "# The more leaves we allow the model to make, the more we move from the underfitting area to the overfitting area.\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of leaves with MAE: \n",
      " {5: 35044.51299744237, 25: 29016.41319191076, 50: 27405.930473214907, 100: 27282.50803885739, 250: 27893.822225701646, 500: 29454.18598068598}\n",
      "\n",
      "Optimal leaf size: 100 with MAE: 27282\n"
     ]
    }
   ],
   "source": [
    "# ==================  Compare Different Tree Sizes  ======================\n",
    "\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "\n",
    "scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\n",
    "print(\"No. of leaves with MAE: \\n\", scores)\n",
    "\n",
    "best_tree_size = min(scores, key = scores.get)\n",
    "print(\"\\nOptimal leaf size: %d with MAE: %d\" %(best_tree_size, scores[best_tree_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================  Fit Model Using All Data  ==================\n",
    "\n",
    "# You know the best tree size. If you were going to deploy this model in practice, you would make \n",
    "# it even more accurate by using all of the data and keeping that tree size.  That is, you don't need \n",
    "# to hold out the validation data now that you've made all your modeling decisions.\n",
    "\n",
    "final_model = DecisionTreeRegressor(max_leaf_nodes = best_tree_size , random_state=0)\n",
    "\n",
    "# fit the final model\n",
    "final_model = final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE when not specifying max_leaf_nodes:  29652.931506849316\n",
      "Validation MAE for best value of max_leaf_nodes:  27282.50803885739\n"
     ]
    }
   ],
   "source": [
    "# run all the cells above to run this cell\n",
    "\n",
    "print(\"Validation MAE when not specifying max_leaf_nodes: \", val_mae)\n",
    "print(\"Validation MAE for best value of max_leaf_nodes: \", scores[best_tree_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for Random Forest Model: 22762.42931506849\n"
     ]
    }
   ],
   "source": [
    "# ======================  Random Forest ========================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the model. Set random_state to 1\n",
    "rf_model = RandomForestRegressor(random_state = 1)\n",
    "\n",
    "# fit your model\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "# predict on validation set\n",
    "val_predictions = rf_model.predict(val_X)\n",
    "\n",
    "# Calculate the mean absolute error of your Random Forest model on the validation data\n",
    "rf_val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE for Decision Tree when not specifying max_leaf_nodes:  29652.931506849316\n",
      "Validation MAE for Decision Tree when best value of max_leaf_nodes:  27282.50803885739\n",
      "Validation MAE for Random Forest Model: 22762.42931506849\n"
     ]
    }
   ],
   "source": [
    "# run all the cells above to run this cell\n",
    "\n",
    "print(\"Validation MAE for Decision Tree when not specifying max_leaf_nodes: \", val_mae)\n",
    "print(\"Validation MAE for Decision Tree when best value of max_leaf_nodes: \", scores[best_tree_size])\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================  when and why to use Random Forest vs Decision Tree  =======================\n",
    "\n",
    "# If the goal is better predictions, we should prefer RF, to reduce the variance.\n",
    "# If the goal is exploratory analysis, we should prefer a single DT , as to understand the \n",
    "# data relationship in a tree hierarchy structure.\n",
    "\n",
    "# Decision Tree is better when the dataset have a “Feature” that is really important to take a decision. \n",
    "# Random Forest, select some “Features” randomly to build the Trees, if a “Feature” is important, \n",
    "# sometimes Random Forest will build trees that will not have the significance that the“Feature” has \n",
    "# in the final decision. I think that Random Forest is good to avoid low quality of data, example: \n",
    "# Imagine a dataset that shows (all houses that doors are green have a high cost), in Decision Trees \n",
    "# this is a bias in the data that can be avoid in Random Forest\n",
    "\n",
    "# If you don't specify the number of trees, the default is 10 trees. Adding more trees generally slightly \n",
    "# increases accuracy, while also increasing computational demands.\n",
    "# In practice, I've commonly seen people specify much larger forests than the default (e.g. 100 trees). \n",
    "# But you hit a point of diminishing returns. You could run even much larger forests then that without\n",
    "# running out of memory. But it is slower.\n",
    "\n",
    "# Decision Tree is more easy than a Random Forest . But a Random Forest makes it easy to work with data \n",
    "# to make a good prediction with large number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============  Make Random Forest on train data and predict on test data ==================\n",
    "\n",
    "# ========================  How to submit on Kaggle ==============================\n",
    "\n",
    "rf_model_on_full_data = RandomForestRegressor()\n",
    "rf_model_on_full_data.fit(X, y)\n",
    "\n",
    "lowa_test_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/test.csv'\n",
    "test_data = pd.read_csv(lowa_test_file_path)\n",
    "test_X = test_data[feature_names]\n",
    "\n",
    "test_preds = rf_model_on_full_data.predict(test_X)\n",
    "\n",
    "# The lines below shows you how to save your data in the format needed to score it in the competition\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': test_preds})\n",
    "\n",
    "# output.to_csv('E:/Data Science/Machine Learning/Iowa Housing Prices/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filepath to variable for easier access\n",
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "# read the data and store data in DataFrame titled lowa_data\n",
    "home_data = pd.read_csv(lowa_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000   \n",
       "\n",
       "           ...         WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count      ...        1460.000000  1460.000000    1460.000000  1460.000000   \n",
       "mean       ...          94.244521    46.660274      21.954110     3.409589   \n",
       "std        ...         125.338794    66.256028      61.119149    29.317331   \n",
       "min        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "25%        ...           0.000000     0.000000       0.000000     0.000000   \n",
       "50%        ...           0.000000    25.000000       0.000000     0.000000   \n",
       "75%        ...         168.000000    68.000000       0.000000     0.000000   \n",
       "max        ...         857.000000   547.000000     552.000000   508.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
       "mean     15.060959     2.758904     43.489041     6.321918  2007.815753   \n",
       "std      55.757415    40.177307    496.123024     2.703626     1.328095   \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000   \n",
       "25%       0.000000     0.000000      0.000000     5.000000  2007.000000   \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000   \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000   \n",
       "max     480.000000   738.000000  15500.000000    12.000000  2010.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1460.000000  \n",
       "mean   180921.195890  \n",
       "std     79442.502883  \n",
       "min     34900.000000  \n",
       "25%    129975.000000  \n",
       "50%    163000.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary of data\n",
    "\n",
    "home_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns in dataset:  81\n",
      "Total Columns in dataset with missing values:  19\n"
     ]
    }
   ],
   "source": [
    "# =====================   Handling Missing Values  ==========================\n",
    "\n",
    "# Python libraries represent missing numbers as nan which is short for \"not a number\". \n",
    "# You can detect which cells have missing values, and then count how many there are in each column with the command\n",
    "\n",
    "array = home_data.values\n",
    "print(\"Total Columns in dataset: \", array.shape[1])\n",
    "\n",
    "missing_val_count_by_column = (home_data.isnull().sum())\n",
    "print(\"Total Columns in dataset with missing values: \", len(missing_val_count_by_column[missing_val_count_by_column > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns in dataset after dropping of columns:  62\n"
     ]
    }
   ],
   "source": [
    "# =============== Method1: A Simple Option: Drop Columns with Missing Values  ================\n",
    "\n",
    "# It can be useful when most values in a column are missing.\n",
    "# Mostly, it is not useful to do this task.\n",
    "\n",
    "data_without_missing_values = home_data.dropna(axis=1)\n",
    "array_without_missing_values = data_without_missing_values.values\n",
    "print(\"Total Columns in dataset after dropping of columns: \", array_without_missing_values.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns in dataset with missing values before imputation:  19\n",
      "Total Columns in dataset with missing values after imputation:  0\n"
     ]
    }
   ],
   "source": [
    "# =================  Method2: Imputation  =================\n",
    "\n",
    "# Imputation fills in the missing value with some number. The imputed value won't be exactly \n",
    "# right in most cases, but it usually gives more accurate models than dropping the column entirely.\n",
    "\n",
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "home_data = pd.read_csv(lowa_file_path) \n",
    "\n",
    "missing_val_count_by_column = (home_data.isnull().sum())\n",
    "print(\"Total Columns in dataset with missing values before imputation: \", len(missing_val_count_by_column[missing_val_count_by_column > 0]))\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "my_imputer = Imputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(home_data.select_dtypes(exclude=['object']).values)   # use only numeric values\n",
    "\n",
    "data_with_imputed_values = pd.DataFrame(data_with_imputed_values)\n",
    "missing_val_count_by_column = (data_with_imputed_values.isnull().sum())\n",
    "print(\"Total Columns in dataset with missing values after imputation: \", len(missing_val_count_by_column[missing_val_count_by_column > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows in dataset before dropping of rows:  1460\n",
      "Total Rows in dataset after dropping of rows:  0\n"
     ]
    }
   ],
   "source": [
    "# =============== Method3: A Simple Option: Drop Rows with Missing Values  ================\n",
    "\n",
    "# Use this method when we have a lot of data and only few rows dropped as a result\n",
    "# In this case, this is not recommended because it reduces (No. of rows redcues to zero)\n",
    "\n",
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "home_data = pd.read_csv(lowa_file_path)\n",
    "\n",
    "array_with_missing_values = home_data.values\n",
    "print(\"Total Rows in dataset before dropping of rows: \", array_with_missing_values.shape[0])\n",
    "\n",
    "data_without_missing_values = home_data.dropna(axis=0)\n",
    "array_without_missing_values = data_without_missing_values.values\n",
    "print(\"Total Rows in dataset after dropping of rows: \", array_without_missing_values.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========  One-Hot Encoding : The Standard Approach for Categorical Data  ============\n",
    "\n",
    "# One hot encoding is the most widespread approach, and it works very well unless your categorical \n",
    "# variable takes on a large number of values (i.e. you generally won't it for variables taking more \n",
    "# than 15 different values. It'd be a poor choice in some cases with fewer values, though that varies.)\n",
    "\n",
    "# =========== Categoricals with Many Values  =============\n",
    "# USE -> Scikit-learn's FeatureHasher uses the hashing trick to store high-dimensional data.\n",
    "\n",
    "# Two approaches can be used and measure the performance of model for benchmark:\n",
    "# 1- One-hot encoded categoricals as well as numeric predictors\n",
    "# 2- Numerical predictors, where we drop categoricals.\n",
    "\n",
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "train_home_data = pd.read_csv(lowa_file_path)\n",
    "train_home_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkLengthOfCategoricalFeaturesOfDataFrame(df, thresh_num):\n",
    "    col_found = False\n",
    "    # get list of dataframe columns to iterate\n",
    "    df_cols = list(train_home_data.columns.values)\n",
    "    for col in df_cols:\n",
    "        # check different no. of values in only categorical features\n",
    "        if(df[col].value_counts().size > thresh_num and df[col].dtype == \"object\"):\n",
    "            col_found = True\n",
    "            print(col)\n",
    "    if(col_found == False):\n",
    "        print(\"No Categorical feature found with more than \" + thresh_num + \" different values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features with more than 15 different values are : \n",
      "\n",
      "Neighborhood\n",
      "Exterior2nd\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical features with more than 15 different values are : \\n\")\n",
    "checkLengthOfCategoricalFeaturesOfDataFrame(train_home_data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "3   4          70         60.0     9550            7            5       1915   \n",
       "4   5          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1          ...            \\\n",
       "0          2003       196.0         706          ...             \n",
       "1          1976         0.0         978          ...             \n",
       "2          2002       162.0         486          ...             \n",
       "3          1970         0.0         216          ...             \n",
       "4          2000       350.0         655          ...             \n",
       "\n",
       "   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0               0             0             0            1   \n",
       "1               0             0             0            1   \n",
       "2               0             0             0            1   \n",
       "3               0             0             0            1   \n",
       "4               0             0             0            1   \n",
       "\n",
       "   SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     0   \n",
       "2                      0                      0                     0   \n",
       "3                      1                      0                     0   \n",
       "4                      0                      0                     0   \n",
       "\n",
       "   SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     0                     1                      0  \n",
       "1                     0                     1                      0  \n",
       "2                     0                     1                      0  \n",
       "3                     0                     0                      0  \n",
       "4                     0                     1                      0  \n",
       "\n",
       "[5 rows x 290 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas offers a convenient function called get_dummies to get one-hot encodings\n",
    "\n",
    "one_hot_encoded_train_home_data = pd.get_dummies(train_home_data)\n",
    "one_hot_encoded_train_home_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 290)\n",
      "(1459, 290)\n"
     ]
    }
   ],
   "source": [
    "# ===========  Applying to Multiple Files  ===============\n",
    "\n",
    "# Ensure the test data is encoded in the same manner as the training data with the align command\n",
    "# The align command makes sure the columns show up in the same order in both datasets (it uses column names \n",
    "# to identify which columns line up in each dataset.) The argument join='left' specifies that we will do \n",
    "# the equivalent of SQL's left join. That means, if there are ever columns that show up in one dataset and \n",
    "# not the other, we will keep exactly the columns from our training data. The argument join='inner' would do \n",
    "# what SQL databases call an inner join, keeping only the columns showing up in both datasets. That's also a sensible choice.\n",
    "\n",
    "train_lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "test_lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/test.csv'\n",
    "train_home_data = pd.read_csv(train_lowa_file_path)\n",
    "test_home_data = pd.read_csv(test_lowa_file_path)\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_home_data)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_home_data)\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n",
    "                                                                    join='left', \n",
    "                                                                    axis=1)\n",
    "print(final_train.shape)\n",
    "print(final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================  XGBoost ========================\n",
    "\n",
    "# XGBoost is the leading model for working with standard tabular data (the type of data you store\n",
    "# in Pandas DataFrames, as opposed to more exotic types of data like images and videos). XGBoost models \n",
    "# dominate many Kaggle competitions.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "data = pd.read_csv(lowa_file_path)\n",
    "# drop only rows where SalePrice is NaN\n",
    "data.dropna(axis = 0, subset = ['SalePrice'], inplace = True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis = 1).select_dtypes(exclude = ['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25, random_state = 1)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(train_X, train_y, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = my_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 15247.674678938356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  ====================  XGBoost Model Tuning  ======================\n",
    "\n",
    "# XGBoost is currently the dominant algorithm for building accurate models \n",
    "# on conventional data (also called tabular or strutured data).\n",
    "\n",
    "# n_estimators specifies how many times to go through the modeling cycle described above.\n",
    "# In the underfitting vs overfitting graph, n_estimators moves you further to the right. \n",
    "# Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. \n",
    "# Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions \n",
    "# on new data (which is what we care about). You can experiment with your dataset to find the ideal. \n",
    "# Typical values range from 100-1000, though this depends a lot on the learning rate discussed below.\n",
    "\n",
    "# The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the \n",
    "# model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. \n",
    "# It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "# Since random chance sometimes causes a \"single round\" where validation scores don't improve, you need to specify \n",
    "# a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a \n",
    "# reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores.\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.09, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1070,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=1,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate is an important hyperparameter for tuning\n",
    "# It is very useful to avoid overfitting of model.\n",
    "# With samll learning_rate, you can use a higher value of n_estimators without overfitting. \n",
    "# If you use early stopping, the appropriate number of trees will be set automatically.\n",
    "\n",
    "# In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, \n",
    "# though it will also take the model longer to train since it does more iterations through the cycle.\n",
    "\n",
    "# On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. \n",
    "# It's common to set the parameter \"n_jobs\" equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "# XGBoost has a multitude of other parameters, but these will go a very long way in helping you fine-tune your XGBoost \n",
    "# model for optimal performance.\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1070, learning_rate=0.09, random_state = 1)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=10, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 15243.358358304795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(test_X)\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================  Criteria for Partial Dependence Plots  =======================\n",
    "\n",
    "# After the model is fit, we could start by taking all the characteristics of a single house. \n",
    "# Say, a house with 2 bedrooms, 2 bathrooms, a large lot, an age of 10 years, etc.\n",
    "# We then use the model to predict the price of that house, but we change the distance variable \n",
    "# before making a prediction. We first predict the price for that house when sitting distance to 4. \n",
    "# We then predict it's price setting distance to 5. Then predict again for 6. And so on. \n",
    "# We trace out how predicted price changes (on the vertical axis) as we move from small values of distance \n",
    "# to large values (on the horizontal axis).\n",
    "\n",
    "# In this description, we used only a single house. But because of interactions, \n",
    "# the partial dependence plot for a single house may be atypical. So, instead we repeat that \n",
    "# mental experiment with multiple houses, and we plot the average predicted price on the vertical axis. \n",
    "# You'll see some negative numbers. That doesn't mean the price would sell for a negative price. \n",
    "# Instead it means the prices would have been less than the actual average price for that distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================  Partial Dependence Plots  =====================\n",
    "\n",
    "# The partial dependence plot is calculated only after the model has been fit.\n",
    "# Partial dependence plots show how each variable or predictor affects the model's predictions.\n",
    "\n",
    "lowa_file_path = 'E:/Data Science/Machine Learning/Iowa Housing Prices/Iowa Dataset/train.csv'\n",
    "data = pd.read_csv(lowa_file_path)\n",
    "# drop only rows where SalePrice is NaN\n",
    "data.dropna(axis = 0, subset = ['SalePrice'], inplace = True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis = 1).select_dtypes(exclude = ['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25, random_state = 1)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "my_model = GradientBoostingRegressor()\n",
    "my_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make partial dependence plots\n",
    "\n",
    "my_plots = plot_partial_dependence(my_model, X = train_X, features = [3, 4], \n",
    "                                   feature_names = data.columns,\n",
    "                                   grid_resolution=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================  Important Steps For Model ==========================\n",
    "\n",
    "# Feature Selection \n",
    "# Data Normalization in case of diff dispersion between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================  Cross Validation  ================\n",
    "\n",
    "# In cross-validation, we run our modeling process on different subsets of the data to get multiple \n",
    "# measures of model quality. For example, we could have 5 folds or experiments. We divide the data \n",
    "# into 5 pieces, each being 20% of the full dataset.\n",
    "\n",
    "# We run an experiment called experiment 1 which uses the first fold as a holdout set, and everything\n",
    "# else as training data. This gives us a measure of model quality based on a 20% holdout set, much as \n",
    "# we got from using the simple train-test split. We then run a second experiment, where we hold out data \n",
    "# from the second fold (using everything except the 2nd fold for training the model.) This gives us a \n",
    "# second estimate of model quality. We repeat this process, using every fold once as the holdout. Putting \n",
    "# this together, 100% of the data is used as a holdout at some point.\n",
    "\n",
    "# Returning to our example above from train-test split, if we have 5000 rows of data, we end up with a measure \n",
    "# of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ==================== Data Leakage  ============================\n",
    "\n",
    "# This is very important to check if your model has a very high accuracy.\n",
    "# In that case, you need to check leakage of data and should exclude some features that leak the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
